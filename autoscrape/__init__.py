# -*- coding: UTF-8 -*-
import logging

from .control import Controller
from .web import Scraper


logger = logging.getLogger('AUTOSCRAPE')


"""
COMMAND        Logical Control Flow Step
--------       -------------------------------------------------------------------
INIT (url)               initialize & get entry point
                                     â”‚
                                     â†“
                                 load page    ğŸ ¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚                            â”‚
SELECT_LINK (index)                  â”‚            click a link based on likelihood
                                     â”‚               of finding a search form
                                     â†“                            â”‚
GET_TAGS     â”Œâ”€â”€â”€â”€ğŸ ¦ look for search form (possibly classifier) â”€â”€â”€â”˜
             â”‚                       â”‚
             â”‚                       â”‚ FOUND
             â”‚                       â†“
GET_TAGS?    â”‚         identify forms on page that require input
             â”‚     (begin with config then move to heuristic then ML)
             â”‚                       â”‚
             â”‚                       â†“
             â”‚      initialize iterators for required inputs
             â”‚      (begin with config/brute force, then RL)
             â”‚                       â”‚
             â”‚                       â†“
             â””â”€â”€â”€â”€â”€â”€â”€ are we at the end of our iterators?
                YES                  â”‚
                                     â†“
INPUT (index, chars)     enter data into form inputs ğŸ ¤â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚                       â”‚
                                     â†“                       â”‚
SUBMIT (index)          submit form and load next page       â”‚
                                     â”‚                       â”‚
                                     â†“                       â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€ğŸ ¦ scrape the page                â”‚
                     â”‚               â”‚                       â”‚
                     â”‚               â†“                       â”‚
GET_TAGS             â”‚     look for a next button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚         (classifier)        NOT FOUND
                     â”‚               â”‚
                     â”‚               â”‚ YES
                     â”‚               â†“
SELECT_LINK (index)  â””â”€â”€â”€ click the next button & load page
"""

class TestScraper(object):

    def __init__(self, baseurl):
        """
        Initialize our scraper and get the first page.
        """
        logger.setLevel(logging.DEBUG)
        console_handler = logging.StreamHandler()
        logger.addHandler(console_handler)

        self.scraper = Scraper()
        self.scraper.fetch(baseurl)
        self.visited_urls = set()

    def run(self, depth=0, tags=None):
        """
        This is the main recursive depth-first search of a site. It
        doesn't do anything but crawl a site DFS and ensure the tagging
        and web engine is working as it should.
        """
        logger.debug("** DEPTH %s" % depth)
        if not tags:
            tags = self.scraper.get_tags()

        logger.debug("All tags at this depth \n    %s" % ("\n    ").join(tags))

        for tag in tags:
            logger.debug("Attempting click on tag \n    %s" % tag)

            if self.scraper.click(tag):
                logger.debug("Clicked! Recursing ...")
                self.run(depth=depth + 1, tags=self.scraper.get_tags())

        logger.debug("Going back...")
        self.scraper.back()


class BruteForceScraper(object):
    pass


class AutoScraper(object):
    pass

